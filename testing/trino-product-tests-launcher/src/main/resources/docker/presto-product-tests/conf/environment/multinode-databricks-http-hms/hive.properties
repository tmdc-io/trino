connector.name=hive
hive.metastore.uri=https://${ENV:DATABRICKS_HOST}:443/api/2.0/unity-hms-proxy/metadata
hive.metastore.http.client.bearer-token=${ENV:DATABRICKS_TOKEN}
hive.metastore.http.client.additional-headers=X-Databricks-Catalog-Name:${ENV:DATABRICKS_UNITY_CATALOG_NAME}
hive.metastore.http.client.authentication.type=BEARER
# We need to give access to bucket owner (the AWS account integrated with Databricks), otherwise files won't be readable from Databricks
hive.s3.upload-acl-type=BUCKET_OWNER_FULL_CONTROL
hive.metastore.thrift.client.ssl.enabled=true
hive.metastore.thrift.client.ssl.trust-certificate=/etc/pki/java/cacerts
hive.non-managed-table-writes-enabled=true
